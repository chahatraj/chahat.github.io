<!DOCTYPE html>
<html lang="en">
<head>
  <title>Publications</title>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
  <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700" rel="stylesheet"/>
  <link href="https://fonts.googleapis.com/css?family=Montserrat:300,400,500,700" rel="stylesheet"/>
  <link href="https://fonts.googleapis.com/css?family=Herr+Von+Muellerhoff" rel="stylesheet"/>
  <link rel="stylesheet" href="css/open-iconic-bootstrap.min.css"/>
  <link rel="stylesheet" href="css/animate.css"/>
  <link rel="stylesheet" href="css/owl.carousel.min.css"/>
  <link rel="stylesheet" href="css/owl.theme.default.min.css"/>
  <link rel="stylesheet" href="css/magnific-popup.css"/>
  <link rel="stylesheet" href="css/aos.css"/>
  <link rel="stylesheet" href="css/ionicons.min.css"/>
  <link rel="stylesheet" href="css/bootstrap-datepicker.css"/>
  <link rel="stylesheet" href="css/jquery.timepicker.css"/>
  <link rel="stylesheet" href="css/flaticon.css"/>
  <link rel="stylesheet" href="css/icomoon.css"/>
  <link rel="stylesheet" href="css/style.css"/>
</head>
<body>
<div id="colorlib-page">
  <a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle"><i></i></a>
  <aside id="colorlib-aside" role="complementary" class="js-fullheight text-center">
    <h1 id="colorlib-logo"><a href="index.html" style="color: #f4a460"><span class="img" style="background-image: url(images/author.jpg)"></span>Chahat Raj</a></h1>
    <nav id="colorlib-main-menu" role="navigation">
      <ul>
        <li class="colorlib-active"><a href="./files/chahat_cv.pdf" style="color: #f4a460">CV</a></li>
        <li class="colorlib-active"><a href="news.html" style="color: #f4a460">News</a></li>
        <li class="colorlib-active"><a href="affiliations.html" style="color: #f4a460">Affiliations</a></li>
        <li class="colorlib-active"><a href="publications.html" style="color: #f4a460">Publications</a></li>
        <li class="colorlib-active"><a href="services.html" style="color: #f4a460">Services</a></li>
        <li class="colorlib-active"><a href="contact.html" style="color: #f4a460">Contact</a></li>
        <ul class="ftco-social mt-3">
          <li class="ftco-animate"><a href="https://linkedin.com/in/chahatraj" style="color: #f4a460"><span class="icon-linkedin"></span></a></li>
          <li class="ftco-animate"><a href="https://facebook.com/chahat.raj.71" style="color: #f4a460"><span class="icon-facebook"></span></a></li>
          <li class="ftco-animate"><a href="https://instagram.com/chatty_chahat" style="color: #f4a460"><span class="icon-instagram"></span></a></li>
          <li class="ftco-animate"><a href="https://twitter.com/chahat__raj" style="color: #f4a460"><span class="icon-twitter"></span></a></li>
        </ul>
      </ul>
    </nav>
  </aside>
  <div id="colorlib-main">
    <div class="author-info text p-3 p-md-5">
      <section class="desc">
        <div class="container">
          <div class="row d-flex">
            <div class="col-lg-32">
              <div class="row">
                <div class="col-md-12">
                  <h1 class="mb-4" style="color: #f4a460"><span></span><b>PUBLICATIONS</b><span></span></h1>
                  <p align="center">
                    <a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=chahatraj&btnG=" target="_blank" class="btn-round btn-primary btn" style="background-color: #f4a460">Google Scholar</a>
                  </p>
                  <br/>
                  <ul>
                    <h1 class="mb-4" style="color: #f4a460" align="left">
                      <span></span><b>Doctoral Research</b><span></span>
                    </h1>
                    <!-- Publication 5 Starts Here -->
                    <li>
                      <p>
                        <a style="color: #000000">BiasDora: Exploring Hidden Biased Associations in Vision-Language Models.</a><br />
                        <a style="color: #f4a460">Chahat Raj</a>, Anjishnu Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, and Ziwei Zhu (Under Review 2024)
                        <button type="button" class="btn badge" style="background-color: #a7c7e7; color: #ffffff" data-popup="absd5">Abstract</button>
                        <a href=""><button type="button" class="btn badge" style="background-color: #ec949c; color: #ffffff">PDF</button></a> <!-- TODO: add pdf -->
                        <a href=""><button type="button" class="btn badge" style="background-color: #ebce77; color: #ffffff">Code</button></a> <!-- TODO: add code -->
                        <button type="button" class="btn badge" style="background-color: #b8d8be; color: #ffffff" data-popup="bibd5">BibTeX</button>
                      </p>
                      <div id="absd5" class="popup-abs" style="display: none; font-size: 12px; text-align: justify;">
                        Existing works examining Vision Language Models (VLMs) for social biases predominantly focus on a limited set of documented bias associations, 
                        such as gender-profession or race-crime. This narrow scope often overlooks a vast range of unexamined implicit associations, 
                        restricting the identification and, hence, mitigation of such biases. We address this gap by probing VLMs to (1) uncover hidden, implicit associations across 
                        9 bias dimensions. We systematically explore diverse input and output modalities and (2) demonstrate how biased associations vary in their negativity, toxicity, 
                        and extremity. Our work (3) identifies subtle and extreme biases that are typically not recognized by existing methodologies. We make the Dataset of retrieved 
                        associations, (Dora), publicly available. <br /> <br />
                      </div>
                      <div id="bibd5" class="popup-bib" style="display: none; font-size: 12px; text-align: left; white-space: pre; padding-top: -20px !important">
                        @article{raj2024biasdora,
                          title={BiasDora: Exploring Hidden Biased Associations in Vision-Language Models.},
                          author={Raj, Chahat and Mukherjee, Anjishnu and Caliskan, Aylin and Anastasopoulos, Antonios and Zhu, Ziwei},
                          journal={},
                          year={2024}
                        }
                      </div>                                            
                    </li>
                    <!-- Publication 5 Ends Here -->
                    <!-- Publication 4 Starts Here -->
                    <li>
                      <p>
                        <a style="color: #000000">Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis.</a><br />
                        <a style="color: #f4a460">Chahat Raj</a>, Anjishnu Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, and Ziwei Zhu (Under Review 2024)
                        <button type="button" class="btn badge" style="background-color: #a7c7e7; color: #ffffff" data-popup="absd4">Abstract</button>
                        <a href=""><button type="button" class="btn badge" style="background-color: #ec949c; color: #ffffff">PDF</button></a> <!-- TODO: add pdf -->
                        <a href=""><button type="button" class="btn badge" style="background-color: #ebce77; color: #ffffff">Code</button></a> <!-- TODO: add code -->
                        <button type="button" class="btn badge" style="background-color: #b8d8be; color: #ffffff" data-popup="bibd4">BibTeX</button>
                      </p>
                      <div id="absd4" class="popup-abs" style="display: none; font-size: 12px; text-align: justify;">
                        Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes
                        and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate 
                        various forms of social contact through LLM prompting to measure their influence on the model's biases, mirroring how intergroup interactions 
                        can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to 
                        measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, 
                        Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM 
                        responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% 
                        in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy. <br /> <br />
                      </div>
                      <div id="bibd4" class="popup-bib" style="display: none; font-size: 12px; text-align: left; white-space: pre; padding-top: -20px !important">
                        @article{raj2024breakingbias,
                          title={Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis.},
                          author={Raj, Chahat and Mukherjee, Anjishnu and Caliskan, Aylin and Anastasopoulos, Antonios and Zhu, Ziwei},
                          journal={},
                          year={2024}
                        }
                      </div>                                            
                    </li>
                    <!-- Publication 4 Ends Here -->
                    <!-- Publication 3 Starts Here -->
                    <li>
                      <p>
                        <a style="color: #000000">SALSA: Salience-Based Switching Attack for Adversarial Perturbations in Fake News Detection Models.</a><br />
                        <a style="color: #f4a460">Chahat Raj*</a>, Anjishnu Mukherjee*, Hemant Purohit, Antonios Anastasopoulos, and Ziwei Zhu (ECIR 2024)
                        <button type="button" class="btn badge" style="background-color: #a7c7e7; color: #ffffff" data-popup="absd3">Abstract</button>
                        <a href="./files/salsa.pdf"><button type="button" class="btn badge" style="background-color: #ec949c; color: #ffffff">PDF</button></a>
                        <a href="https://github.com/iamshnoo/salsa"><button type="button" class="btn badge" style="background-color: #ebce77; color: #ffffff">Code</button></a>
                        <button type="button" class="btn badge" style="background-color: #b8d8be; color: #ffffff" data-popup="bibd3">BibTeX</button>
                      </p>
                      <div id="absd3" class="popup-abs" style="display: none; font-size: 12px; text-align: justify;">
                        Despite advances in fake news detection algorithms, recent research reveals that machine learning-based fake news detection models are still 
                        vulnerable to carefully crafted adversarial attacks. In this landscape, traditional methods, often relying on text perturbations or
                        heuristic-based approaches, have proven insufficient, revealing a critical need for more nuanced and context-aware strategies to enhance the 
                        robustness of fake news detection. Our research identifies and addresses three critical areas: creating subtle perturbations, preserving core 
                        information while modifying sentence structure, and incorporating inherent interpretability. We propose SALSA, an adversarial Salience-based
                        Switching Attack strategy that harnesses salient words, using similaritybased switching to address the shortcomings of traditional adversarial
                        attack methods. Using SALSA, we perform a two-way attack: misclassifying real news as fake and fake news as real. Due to the absence of
                        standardized metrics to evaluate adversarial attacks in fake news detection, we further propose three new evaluation metrics to gauge the attack's 
                        success. Finally, we validate the transferability of our proposed attack strategy across attacker and victim models, demonstrating our
                        approach's broad applicability and potency. <br /> <br />
                      </div>
                      <div id="bibd3" class="popup-bib" style="display: none; font-size: 12px; text-align: left; white-space: pre; padding-top: -20px !important">
                        @inproceedings{raj2024salsa,
                          title={SALSA: Salience-Based Switching Attack for Adversarial Perturbations in Fake News Detection Models},
                          author={Raj, Chahat and Mukherjee, Anjishnu and Purohit, Hemant and Anastasopoulos, Antonios and Zhu, Ziwei},
                          booktitle={European Conference on Information Retrieval},
                          pages={35--49},
                          year={2024},
                          organization={Springer}
                        }
                      </div>                                            
                    </li>
                    <!-- Publication 3 Ends Here -->
                    <!-- Publication 2 Starts Here -->
                    <li>
                      <p>
                        <a style="color: #000000">Global Voices, Local Biases: Socio-Cultural Prejudices across Languages.</a><br />
                        Anjishnu Mukherjee*, <a style="color: #f4a460">Chahat Raj*</a>, Ziwei Zhu, and Antonios Anastasopoulos (EMNLP 2023)
                        <button type="button" class="btn badge" style="background-color: #a7c7e7; color: #ffffff" data-popup="absd2">Abstract</button>
                        <a href="https://aclanthology.org/2023.emnlp-main.981.pdf"><button type="button" class="btn badge" style="background-color: #ec949c; color: #ffffff">PDF</button></a>
                        <a href="https://github.com/iamshnoo/weathub"><button type="button" class="btn badge" style="background-color: #ebce77; color: #ffffff">Code</button></a>
                        <button type="button" class="btn badge" style="background-color: #b8d8be; color: #ffffff" data-popup="bibd2">BibTeX</button>
                      </p>
                      <div id="absd2" class="popup-abs" style="display: none; font-size: 12px; text-align: justify;">
                        Human biases are ubiquitous but not uniform: disparities exist across linguistic, cultural, and societal borders. As large amounts of recent 
                        literature suggest, language models (LMs) trained on human data can reflect and often amplify the effects of these social biases. However, 
                        the vast majority of existing studies on bias are heavily skewed towards Western and European languages. In this work, we scale the Word 
                        Embedding Association Test (WEAT) to 24 languages, enabling broader studies and yielding interesting findings about LM bias. We additionally 
                        enhance this data with culturally relevant information for each language, capturing local contexts on a global scale. Further, to encompass 
                        more widely prevalent societal biases, we examine new bias dimensions across toxicity, ableism, and more. Moreover, we delve deeper into the 
                        Indian linguistic landscape, conducting a comprehensive regional bias analysis across six prevalent Indian languages. Finally, we highlight 
                        the significance of these social biases and the new dimensions through an extensive comparison of embedding methods, reinforcing the need to 
                        address them in pursuit of more equitable language models. <br /> <br />
                      </div>
                      <div id="bibd2" class="popup-bib" style="display: none; font-size: 12px; text-align: left; white-space: pre; padding-top: -20px !important">
                        @inproceedings{mukherjee-etal-2023-global,
                          title = "{G}lobal {V}oices, Local Biases: Socio-Cultural Prejudices across Languages",
                          author = "Mukherjee, Anjishnu  and Raj, Chahat  and Zhu, Ziwei  and Anastasopoulos, Antonios",
                          editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
                          booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
                          month = dec,
                          year = "2023",
                          address = "Singapore",
                          publisher = "Association for Computational Linguistics",
                          url = "https://aclanthology.org/2023.emnlp-main.981",
                          doi = "10.18653/v1/2023.emnlp-main.981",
                          pages = "15828--15845",
                      }
                      </div>                                            
                    </li>
                    <!-- Publication 2 Ends Here -->
                    <!-- Publication 1 Starts Here -->
                    <li>
                      <p>
                        <a style="color: #000000">True and Fair: Robust and Unbiased Fake News Detection via Interpretable Machine Learning.</a><br />
                        <a style="color: #f4a460">Chahat Raj</a>, Anjishnu Mukherjee and Ziwei Zhu (AAAI/ACM AIES 2023)
                        <button type="button" class="btn badge" style="background-color: #a7c7e7; color: #ffffff" data-popup="absd1">Abstract</button>
                        <a href="./files/trueandfair.pdf"><button type="button" class="btn badge" style="background-color: #ec949c; color: #ffffff">PDF</button></a>
                        <a href="https://github.com/chahatraj/true-and-fair"><button type="button" class="btn badge" style="background-color: #ebce77; color: #ffffff">Code</button></a>
                        <button type="button" class="btn badge" style="background-color: #b8d8be; color: #ffffff" data-popup="bibd1">BibTeX</button>
                      </p>
                      <div id="absd1" class="popup-abs" style="display: none; font-size: 12px; text-align: justify;">
                        The dissemination of information, and consequently, misinformation, occurs at an unprecedented speed, making it increasingly difficult to 
                        discern the credibility of rapidly circulating news. Advanced large-scale language models have facilitated the development of classifiers 
                        capable of effectively identifying misinformation. Nevertheless, these models are intrinsically susceptible to biases that may be introduced 
                        through numerous ways, including contaminated data sources or unfair training methodologies. When trained on biased data, machine learning 
                        models may inadvertently learn and reinforce these biases, leading to reduced generalization performance. This situation consequently results 
                        in an inherent "unfairness" within the system. Interpretability, referring to the ability to understand and explain the decision-making 
                        process of a model, can be used as a tool to explain these biases. Our research aims to identify the root causes of these biases in fake news 
                        detection and mitigate their presence using interpretability. We also perform inference time attacks to fairness to validate robustness. <br /> <br />
                      </div>
                      <div id="bibd1" class="popup-bib" style="display: none; font-size: 12px; text-align: left; white-space: pre; padding-top: -20px !important">
                        @inproceedings{10.1145/3600211.3604760,
                          author = {Raj, Chahat and Mukherjee, Anjishnu and Zhu, Ziwei},
                          title = {True and Fair: Robust and Unbiased Fake News Detection via Interpretable Machine Learning},
                          year = {2023},
                          isbn = {9798400702310},
                          publisher = {Association for Computing Machinery},
                          address = {New York, NY, USA},
                          url = {https://doi.org/10.1145/3600211.3604760},
                          doi = {10.1145/3600211.3604760},
                          booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
                          pages = {962-963},
                          numpages = {2},
                          keywords = {security, misinformation, interpretability, fairness, bias},
                          location = {<conf-loc>, <city>Montr\'{e}al</city>, <state>QC</state>, <country>Canada</country>, </conf-loc>},
                          series = {AIES '23}
                          }
                      </div>                                            
                    </li>
                    <!-- Publication 1 Ends Here -->
                      <br />
                      <script src="js/jquery.min.js"></script>
                      <script src="js/jquery-migrate-3.0.1.min.js"></script>
                      <script src="js/popper.min.js"></script>
                      <script src="js/bootstrap.min.js"></script>
                      <script src="js/jquery.easing.1.3.js"></script>
                      <script src="js/jquery.waypoints.min.js"></script>
                      <script src="js/jquery.stellar.min.js"></script>
                      <script src="js/owl.carousel.min.js"></script>
                      <script src="js/jquery.magnific-popup.min.js"></script>
                      <script src="js/aos.js"></script>
                      <script src="js/jquery.animateNumber.min.js"></script>
                      <script src="js/bootstrap-datepicker.js"></script>
                      <script src="js/jquery.timepicker.min.js"></script>
                      <script src="js/scrollax.min.js"></script>
                      <script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyBVWaKrjvy3MaE7SQ74_uJiULgl1JY0H2s&sensor=false"></script>
                      <script src="js/google-map.js"></script>
                      <script src="js/main.js"></script>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </section>
        </div>
      </div>
    </div>
    <script>
      document.addEventListener('DOMContentLoaded', function() {
        var badges = document.querySelectorAll("[data-popup]");
        badges.forEach(function(badge) {
          var popup = document.getElementById(badge.dataset.popup);
          badge.addEventListener("click", function() {
            if (popup.style.display === "none") {
              popup.style.display = "block"; // Show the popup
            } else {
              popup.style.display = "none"; // Hide the popup
            }
          });
        });
      });
    </script> 
  </body>
</html>
